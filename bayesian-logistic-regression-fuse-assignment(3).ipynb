{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bayesian Logistic Regression\n\n---\n\n<b><div style=\"text-align: right\">[TOTAL POINTS: 20]</div></b>\n\nIn previous reads of this unit, you learned bayesian approach for parameter estimation on linear regression model and MLE for logistic regression. In this assignment, your task is to build a __bayesian logistic regression model__ on binary classification problem. Don't fret, all the theory related to bayesian parameter estimation for logistic regression is present alongside the code blocks. Let's get started.\n\n## Learning Objective\n\nBy the end of this assignment, student should be able to:\n\n- Develop theoritical understanding of bayesian logistic regression.\n\n- Implement functions to compute negative log posterior, normalized gradient and hessian matrix, required for computing bayesian logistic regression.\n\n- Finally, use above mentioned base functions to implement bayesian logistic regression model.","metadata":{"deletable":false,"id":"p9phsHOLDFPd","nbgrader":{"cell_type":"markdown","checksum":"5d993880e28d973afbd161eebabb1499","grade":false,"grade_id":"cell-e1e946de39c15485","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"markdown","source":"## Basic Imports and Settings","metadata":{"deletable":false,"id":"7YWVzlq8DFPg","nbgrader":{"cell_type":"markdown","checksum":"68d372953571cef521fa8630afdb09bb","grade":false,"grade_id":"cell-cc6b895ec950a3cc","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"# Importing required libraries for data generation and visualization\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sys\n\n# MPL settings\nplt.rcParams.update({\n    'figure.figsize':(12,8),\n    'font.size': 16,\n})\n\n# Defining random state of variable\nRANDOM_STATE=100","metadata":{"deletable":false,"id":"gwsDSA-vDFPh","nbgrader":{"cell_type":"code","checksum":"dd0870417c5042181ffc9827df3e34f8","grade":false,"grade_id":"cell-3653fdc5ee1456ca","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset\n\n### Exercise 1: Generating Synthetic Dataset\n\n<b><div style=\"text-align: right\">[POINTS: UNGRADED]</div></b>\n\nThe logistic regression model created in this assignment is trained and tested on a _synthetic classification dataset_. Your first task is regarding the generation of synthetic dataset.\n\n__Task:__\n\n1. Import ```sklearn.datasets.make_classification``` method. \n\n2. Generate a synthetic binary classification dataset using ```sklearn's make_classifcation``` method. Store the independent features in variable ```X_data``` and target features in ```y_data```.\n\n  The characteristics of the data or parameter passed are as:\n  - Number of samples = 300\n  - Number of features = 2\n  - Number of informative features = 2\n  - Number of redundant features = 0\n  - ```shift``` = 50\n  - ```class_sep``` = 1.2\n\nNOTE: Use the ```RANDOM_STATE``` constant as random_state when generating the synthetic data in ```make_classification``` method.\n\n3. Next, Store the dataset in a Pandas DataFrame named ```syn_df```. Each column must be a feature or target value. The column names must be ```feature_1, feature_2,``` and ```target``` for independent features and target, respectively.\n\nThe ```X_data, y_data``` and ```syn_df``` must be of shape (300,2), (300,) and (300,3) respectively, with the number of samples ($n$)=300 number of features ($m$) = 2.","metadata":{"deletable":false,"id":"0qbnhfFBDFPm","nbgrader":{"cell_type":"markdown","checksum":"7cab822456993720ebb7afd96468f11e","grade":false,"grade_id":"cell-fe492237fdafe809","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"# Variables to store the dataset\nX_data, y_data, syn_df = None, None, None","metadata":{"deletable":false,"id":"9CAwa2Z6DFPm","nbgrader":{"cell_type":"code","checksum":"77c27dfa94b39d458482b760d12678f7","grade":false,"grade_id":"cell-21f885fa2746b994","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Ex-1-Task-1\n\n### BEGIN SOLUTION\n# your code here\nfrom sklearn.datasets import make_classification\nX_data,y_data=make_classification(n_samples=300, n_features=2,n_informative=2,n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2,class_sep=1.2,shift=50,random_state=RANDOM_STATE)\n#y_data=y_data.reshape(-1,1)\nval=np.concatenate((X_data, y_data.reshape(-1,1)), axis=1)\nsyn_df = pd.DataFrame(data=val,columns=['feature_1','feature_2','target'])\n#raise NotImplementedError\n### END SOLUTION","metadata":{"deletable":false,"id":"FMb7mYUqDFPq","nbgrader":{"cell_type":"code","checksum":"12fcf1f40f5c7d34e074608f440a8876","grade":false,"grade_id":"cell-994cdc00586fa643","locked":false,"schema_version":3,"solution":true},"scrolled":true,"tags":["Ex-1-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert X_data is not None and y_data is not None and syn_df is not None,\\\n'The feature matrix and/or target vector and/or dataframe is empty '\n\nassert X_data.shape == (300,2), 'Wrong dimension of input feature matrix'\nassert y_data.shape == (300,), 'Wrong dimension of target vector'\nassert syn_df.shape == (300,3), 'Wrong dimension of dataframe'\n","metadata":{"deletable":false,"id":"BqTCQz86DFPu","nbgrader":{"cell_type":"code","checksum":"0ed27f9ea450cd0f64033c0029dddb1f","grade":true,"grade_id":"cell-0f3f033e478951fd","locked":true,"points":0,"schema_version":3,"solution":false},"tags":["Ex-1-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part 1: EDA and preprocessing\n\nStarting with the EDA, we require to know about the number of samples, features/target and type. Using ```.info()``` property display all the above mentioned properties of the dataset. ","metadata":{"deletable":false,"id":"O96y0rzLDFPx","nbgrader":{"cell_type":"markdown","checksum":"fbcd401eba889f1a4b17b92d4fac9eaf","grade":false,"grade_id":"cell-6519cebbbce1106e","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"# Information regarding the synthetic data\nsyn_df.info()","metadata":{"deletable":false,"id":"pdw5OrunDFPy","nbgrader":{"cell_type":"code","checksum":"80ea476359f7a94e55812e0c111c5137","grade":false,"grade_id":"cell-0d74757336db5a03","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see synthetic dataset generated contains only two numeric feature. Which of the following should be conducted for this data?\n\na. categorical feature transformation.\n\nb. feature selection. so performing \n\n<details>    \n<summary>\n    <font size=\"3\" color=\"darkgreen\"><b> Answer</b></font>\n</summary>\n<p>     \n<ul>\n</ul>\n</p>\n\n- None of them. No proper reason to conduct categorical variable transformation on an all numeric dataset. Also with only two features, feature selection process is unneccesary. \n\nNext, let's look if the data is imbalanced.","metadata":{"deletable":false,"id":"NU9IDjhADFP2","nbgrader":{"cell_type":"markdown","checksum":"9d25266c6576498df15924749f47388d","grade":false,"grade_id":"cell-7bb12a92abd2ee78","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"# Checking count of samples with respect to class\nsyn_df['target'].value_counts()","metadata":{"deletable":false,"id":"QV4NBALiDFP3","nbgrader":{"cell_type":"code","checksum":"6528cbc1eebce57dbf8522725ce6aa31","grade":false,"grade_id":"cell-529e635c5c74a447","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected the synthetic dataset generated using sklearn is __balanced__. Which of the following evaluation metric can be used for this classification problem?\n\na. Accuracy.\n\nb. F1-score.\n\n<details>    \n<summary>\n    <font size=\"3\" color=\"darkgreen\"><b> Answer</b></font>\n</summary>\n<p>     \n<ul>\n</ul>\n</p>\n\n- Both of them. As there is no imbalance in the data, both accuracy and f1-score evaluates. (No misleading values.)\n\nNext, let's look at the distribution of the features.","metadata":{"deletable":false,"id":"Ial_eX2bDFP7","nbgrader":{"cell_type":"markdown","checksum":"28be93acbae6bc35b1257a8e22a3d1c5","grade":false,"grade_id":"cell-6dc7ea3f0e3fcf42","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"# Distribution of the features\n\nsyn_df[['feature_1', 'feature_2']].hist(figsize=(12,4), bins='auto')\nplt.show()","metadata":{"deletable":false,"id":"dQYoc9qzDFP8","nbgrader":{"cell_type":"code","checksum":"3f77f1edfff905dcab0b1f3c7fc49599","grade":false,"grade_id":"cell-bd05ed6403f18a8c","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Normally distributed features results in better performance of logistic regression model. Mathematical transformation techniques are viable for changing the feature distribution. However, let's scatterplot the dataset and to check if the transformation is necessary.","metadata":{"deletable":false,"id":"vmbcY6P1DFP_","nbgrader":{"cell_type":"markdown","checksum":"f8f45b521b016873ea6591baa3e8b319","grade":false,"grade_id":"cell-2ac6eb60d12d36fa","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"# Visualization\nplt.figure(figsize=(12,8))\n# Plotting class 0 data\nplt.plot(X_data[:,0][y_data==0], X_data[:,1][y_data==0], 'bs', label='Class 0')\n\n# Plotting class 1 data\nplt.plot(X_data[:,0][y_data==1], X_data[:,1][y_data==1], 'g^', label='Class 1')\n\n# labeling plot\nplt.xlabel(syn_df.columns[0])\nplt.ylabel(syn_df.columns[1])\nplt.title('Synthetic binary classification dataset')\nplt.legend()\nplt.show()","metadata":{"deletable":false,"id":"7tCVZqCrDFQA","nbgrader":{"cell_type":"code","checksum":"c97973164d33c0ce15a034ea92d2676d","grade":false,"grade_id":"cell-00e61927678282da","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above visualization, we find the dataset being __separable by a linear line__, i.e. separable using logistic regression model. As the dataset is already separable by logistic regression model, mathematical transformation isn't necessary.\n\nHowever, We require converting the data into correct dimension. To be exact, padded input feature matrix,\n$\n \\it{X} = \n\\begin{pmatrix}\n\\mathbf{x}_1^T  \\\\ \n\\mathbf{x}_2^T \\\\\n.\\\\. \\\\.\\\\\n\\mathbf{x}_n^T  \\\\\n\\end{pmatrix} =\n \\begin{pmatrix}\n  1 & x_{1,1} & \\cdots & x_{1,m} \\\\\n  1 & x_{2,1} & \\cdots & x_{2,m} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  1 & x_{n,1} & \\cdots & x_{n,m}\n \\end{pmatrix}_{\\text{(n $\\times$ (m+1))}}$, and target vector\n$\\mathbf{y}=\\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{pmatrix}_{\\text{(n $\\times$ 1)}}$  for effective vector computation hypothesis for logistic regression model.\n\nAlso, though this synthetic data has feature magnitudes around same range, you will be conducting feature scaling. Discussion on why the features are scaled is present in the last section of this assignment.","metadata":{"deletable":false,"id":"cfXwRdPqDFQD","nbgrader":{"cell_type":"markdown","checksum":"4422ed3001a85ec0d87dae4f7f9239fa","grade":false,"grade_id":"cell-43b94e9e00578471","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"markdown","source":"### Exercise 2: Feature scaling and Conversion the data into proper dimension plus train-test split\n\n<b><div style=\"text-align: right\">[POINTS: 2]</div></b>\n\nLogistic regression is prone to feature magnitude, scaling the features is essential. In this exercise you are doing the following:\n\n- __Tasks__\n\n __[Task 1: Points: 1]__\n 1. Scale the feature matrix, store in ```X_mat_norm```. Use ```sklearn.preprocessing.MinMaxScaler``` for scaling.\n \n __[Task 2: Points: 1]__\n 2. Pad the feature matrix ```X_mat_norm``` with column of ones, and store in ```X_mat_pad```. Reshape the dimension target vector ```y_data``` and store in ```y_vec```. \n 3. Finally, split the dataset with test_size = 0.2.","metadata":{"deletable":false,"id":"VhW4dm05DFQE","nbgrader":{"cell_type":"markdown","checksum":"091f9ab561e5bdcfdb0d5212982d4394","grade":false,"grade_id":"cell-1645c9ea03057b3f","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"## Variable to store the scaled feature vector.\nX_mat_norm = None\n\n## Variables to store the matrix and vector.\nX_mat_pad, y_vec = None, None\n\n## Variable to store train test data\nX_train, X_test, y_train, y_test = None, None, None, None","metadata":{"deletable":false,"id":"dr2Gl4y6DFQE","nbgrader":{"cell_type":"code","checksum":"a99c076bf7343d31e0f9363de27116f2","grade":false,"grade_id":"cell-88abbc4ffe4afd09","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Ex-2-Task-1\n\n### BEGIN SOLUTION\n# your code here\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nscaler = MinMaxScaler()\nX_mat_norm = scaler.fit_transform(X_data)\n#X_mat_pad = np.pad(X_mat_norm)\n#raise NotImplementedError\n### END SOLUTION","metadata":{"deletable":false,"id":"eIf-q457DFQH","nbgrader":{"cell_type":"code","checksum":"248394f7114ca710e1cdd552e1f410c6","grade":false,"grade_id":"cell-2b51d5c28530adf1","locked":false,"schema_version":3,"solution":true},"tags":["Ex-2-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert X_mat_norm is not None, 'X_mat_norm is empty'\n","metadata":{"deletable":false,"id":"ZEInFSkHDFQO","nbgrader":{"cell_type":"code","checksum":"d44ec70d6559bebfe3ff8e9f591287d1","grade":true,"grade_id":"cell-9e001aae97f4db79","locked":true,"points":0,"schema_version":3,"solution":false},"tags":["Ex-2-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Ex-2-Task-2\n\n### BEGIN SOLUTION\n# your code here\ny_vec = y_data.reshape(-1,1)\nX_mat_pad = np.pad(X_mat_norm, [(0, 0), (1, 0)], mode='constant',constant_values=1)\nX_train, X_test, y_train, y_test = train_test_split(X_mat_pad, y_vec, test_size=0.2)\n#raise NotImplementedError\n### END SOLUTION","metadata":{"deletable":false,"id":"dpDDmpr_DFQR","nbgrader":{"cell_type":"code","checksum":"4229ead8e454e3d4e27e2efaa84c12f3","grade":false,"grade_id":"cell-486ac27fa99a0018","locked":false,"schema_version":3,"solution":true},"tags":["Ex-2-Task-2"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert X_mat_pad is not None and y_vec is not None , 'X_mat_pad or y_vec is empty'\nassert X_mat_pad.shape == (300,3) and y_vec.shape == (300,1), 'Wrong shape'\n","metadata":{"deletable":false,"id":"HBmEGxOeDFQU","nbgrader":{"cell_type":"code","checksum":"b8d3de9ec6bfae8e25eb0a142b00ba7e","grade":true,"grade_id":"cell-d9bcefbd04ebb3bd","locked":true,"points":1,"schema_version":3,"solution":false},"tags":["Ex-2-Task-2"]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Changes after scaling and train_test_split","metadata":{"deletable":false,"id":"3iDVAXvZDFQX","nbgrader":{"cell_type":"markdown","checksum":"0b8d88e795938d5d17670bf4bba48ffd","grade":false,"grade_id":"cell-99db3247951bf729","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"# Visualization after scaling\n\nplt.figure(figsize=(12,8))\n# Plotting class 0 data\nplt.plot(X_mat_pad[:,1][y_data==0], X_mat_pad[:,2][y_data==0], 'bs', label='Class 0')\n\n# Plotting class 1 data\nplt.plot(X_mat_pad[:,1][y_data==1], X_mat_pad[:,2][y_data==1], 'g^', label='Class 1')\n\n# labeling plot\nplt.xlabel(syn_df.columns[0])\nplt.ylabel(syn_df.columns[1])\nplt.title('Synthetic binary classification dataset, scaled')\nplt.legend()\nplt.show()","metadata":{"deletable":false,"id":"IpCQMUXnDFQY","nbgrader":{"cell_type":"code","checksum":"35eb7a076e919dc02f9ba2cdd77c4043","grade":false,"grade_id":"cell-11253c0097369842","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After minmax scaling, notice that range of feature value is between 0 and 1. However, the shape of data clusters hasn't changed. Next, let's see the class label count after train test split for both training and test set.","metadata":{"deletable":false,"id":"pOpzSXK-DFQb","nbgrader":{"cell_type":"markdown","checksum":"22d8613c2964129ae1857beb9214b76d","grade":false,"grade_id":"cell-511717631be06f38","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"plt.figure(figsize=(18,5))\nplt.subplot(121)\nplt.hist(y_train)\nplt.title('Count of class in train set')\nplt.subplot(122)\nplt.hist(y_test)\nplt.title('Count of class in test set')","metadata":{"deletable":false,"id":"HzP8D5IwDFQb","nbgrader":{"cell_type":"code","checksum":"530e8e02ba299dbd10b633d61b420c37","grade":false,"grade_id":"cell-4740de5db0f66824","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen in the histograms - no imbalance of class in either train or test set. Thus, stratified sampling isn't required. This concludes EDA and preprocessing part of the assignment.","metadata":{"deletable":false,"id":"yrgJDtgJDFQe","nbgrader":{"cell_type":"markdown","checksum":"9b222197f4ff3c398c0c2984452ef175","grade":false,"grade_id":"cell-c346a2cb646cbe75","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"markdown","source":"## Part 2: Implementation of bayesian logistic regression\n\nThis section contains the theory and implemenation portion of bayesian logistic regression model. Your task is to implement multiple functions required for developing bayesian logistic regression model. It's highly advised that you __read the theory portions__ present above the implementation throughly before starting to code.","metadata":{"deletable":false,"id":"MKnOD3rDDFQf","nbgrader":{"cell_type":"markdown","checksum":"44e3c996f3170578dc398ec9d3e126fe","grade":false,"grade_id":"cell-658a50d5724bcd11","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"markdown","source":"### Bayesian inference for logistic regression\n\nFirst, let's assume we have a binary classification dataset given by $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i), i \\in \\{1, \\cdots, n\\}\\}, y_i \\in \\{0,1\\}$. As a quick review, logistic regression gives the probability of positive class given feature vector computed using __hypothesis function__ as:\n\n$$\n\\boxed{P(y_i=1|\\mathbf{x}_i,\\boldsymbol \\beta)= h_i = \\underbrace{\\frac{e^{(\\mathbf{x}_i^T\\boldsymbol \\beta)}}{1+e^{(\\mathbf{x}_i^T\\boldsymbol \\beta)}}}_{\\sigma(\\mathbf{x})} = \\frac{1}{1+e^{-(\\mathbf{x}_i^T\\boldsymbol \\beta)}}} \\tag{1}\n$$\n\nWhere, $\\beta$ is the parameter of the model, $y_i$ and $\\mathbf{x}_i$ is the $i^{th}$ true target class and covariate vector, and $\\sigma(x)$ is the sigmoid function.\n\nUnlike point estimation method such as MLE and MAP, bayesian estimation gives the probability distribution over the parameter $\\boldsymbol \\beta$, i.e. posterior distribution. Next, for the  computation the posterior distribution we use bayes theorem as:\n\n$$\n\\boxed{p(\\boldsymbol \\beta \\vert \\mathcal{D}) = \\frac{p(\\mathcal D \\vert \\boldsymbol \\beta ) p(\\boldsymbol \\beta) }{\\underbrace{p(\\mathcal D)}_{\\text{constant}} }\\propto p(\\mathcal D \\vert \\boldsymbol \\beta ) p(\\boldsymbol \\beta)} \\tag{2}\n$$\n\nOr,\n\n$$\n\\text{Posterior} = \\text{Constant} \\times \\text{Likelihood} \\times \\text{Prior}\n$$\n\nWhere, the constant normalization term, __evidence__ is given as:\n\n$$\n\\boxed{P(\\mathcal{D}) = \\int_{\\boldsymbol \\beta' \\in B} p(\\mathcal D \\vert \\boldsymbol \\beta' ) p(\\boldsymbol \\beta') d\\boldsymbol \\beta'} \\tag{3}\n$$\n\nAlong with that, for prediction of future data points one require computing the __predictive distribution__ $p(y_i \\vert \\mathcal{D})$, written in the form of marginalization over paramters $\\boldsymbol \\beta$ as:\n\n$$\n\\boxed{p(y_i \\vert \\mathcal D) = \\int p(y_i \\vert \\mathbf{x}_i, \\boldsymbol \\beta) \\ p(\\boldsymbol \\beta \\vert \\mathcal D) \\ d\\boldsymbol \\beta} \\tag{4}\n$$\n\nUnfortunately, __bayesian inference for logistic regression model is intractable__. Precisely, evaluation of posterior distribution requires computing the normalization (evidence) term comprising the likelihood function and prior. Since, the __likelihood function__ $P(\\mathbf{y}|\\it{X}, \\boldsymbol \\beta)$ of logistic regression model  given in terms of __bernoulli distribution__ as,\n\n$$\n\\boxed{p(\\mathcal D \\vert \\boldsymbol \\beta ) = p(\\mathbf{y} \\vert \\it{X}, \\boldsymbol \\beta) =  \\prod_{i=1}^n p(y_i \\vert \\mathbf{x}_i, \\boldsymbol \\beta)\n= \\prod_{i=1}^nh_i^{y_i}(1-h_i)^{1-y_i}} \\tag{5} \n$$\n\ncomprises of product of sigmoid functions (transcendental function) for every data point, there is no closed-form solution for the evidence. Consequently, there's no closed-form solution for posterior distribution and also the predictive function.\n\nNote: The dimension for weight vector is:\n\n$$\\boldsymbol \\beta = \\begin{pmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_m \n\\end{pmatrix}_{\\text{((m+1) $\\times$ 1)}},\n$$\n\n","metadata":{"deletable":false,"id":"3emF_-aBDFQg","nbgrader":{"cell_type":"markdown","checksum":"2f11bcd1581ce6a29d3269f6ff96504a","grade":false,"grade_id":"cell-2335cab94efccb2a","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"markdown","source":"### Exercise 3: Define functions for calculating sigmoid and hypothesis function\n\n<b><div style=\"text-align: right\">[POINTS: 2]</div></b>\n\n**Task:** \n- Define a lambda function to compute sigmoid value given an input ```x```. Store the function in ```sigmoid```. __[Task 1: Points:1]__\n- Define a function named ```calculate_hypothesis``` to compute the hypothesis function of logistic regression model. __[Task 2: Points:1]__\n\nNote: the hypothesis of logistic regression model must be on shape ```(n,1)```.","metadata":{"deletable":false,"id":"VFKKkSZ4DFQh","nbgrader":{"cell_type":"markdown","checksum":"e7a331342539929012d553400096d231","grade":false,"grade_id":"cell-db6e6d809db0b98d","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"# Variable to store sigmoid function\nsigmoid = None","metadata":{"deletable":false,"id":"IQ6T_D4zDFQh","nbgrader":{"cell_type":"code","checksum":"492d952003a7d192d74769e9627d0261","grade":false,"grade_id":"cell-158063b2eeec675e","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Ex-3-Task-1\n\n### BEGIN SOLUTION\n# your code here\nsigmoid = lambda x: 1 /(1 + 1 / np.exp(x))\n#raise NotImplementedError\n### END SOLUTION","metadata":{"deletable":false,"id":"tG8BWDHODFQk","nbgrader":{"cell_type":"code","checksum":"e3176e6dfbd0da46f12fefbc605cf86b","grade":false,"grade_id":"cell-177c51b0059c5e84","locked":false,"schema_version":3,"solution":true},"tags":["Ex-3-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert sigmoid is not None, 'Please initialize sigmoid function'\n","metadata":{"deletable":false,"id":"4fmj3dpqDFQn","nbgrader":{"cell_type":"code","checksum":"bd165b080bdaad2f9ffcfdd4578f20e8","grade":true,"grade_id":"cell-da2374029c091107","locked":true,"points":1,"schema_version":3,"solution":false},"tags":["Ex-3-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Ex-3-Task-2\ndef calculate_hypothesis(input_feat, beta):\n    '''\n    Computes the hypothesis function of logistic regression model\n    \n    Parameters\n    ----------\n    input_feat: np.array, shape:(n,m+1)\n                padded input matrix\n        \n    beta: np.array, shape: (m+1,1)\n           weight coefficients vector\n        \n    Returns\n    -------\n    hyp: np.array, shape:(n,1)\n            hypothesis/predicted value of logistic regression model\n    '''\n    ### BEGIN SOLUTION\n    # your code here\n    hyp = sigmoid(np.dot(np.array(input_feat),np.array(beta)))\n    #raise NotImplementedError\n    ### END SOLUTION\n    \n    return hyp","metadata":{"deletable":false,"id":"MmGy2GwjDFQq","nbgrader":{"cell_type":"code","checksum":"fd65f050898843ba43ac7bc2ae81192a","grade":false,"grade_id":"cell-85916135f1a1a0e9","locked":false,"schema_version":3,"solution":true},"tags":["Ex-3-Task-2"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert calculate_hypothesis(np.ones((5,2)), np.ones((2,1))).shape == (5,1), 'wrong dimension of the hypothesis vector'\n","metadata":{"deletable":false,"id":"xXQHYRifDFQs","nbgrader":{"cell_type":"code","checksum":"32b760ff913b188434c057ada1deda18","grade":true,"grade_id":"cell-dff4ca50e5f45bad","locked":true,"points":1,"schema_version":3,"solution":false},"tags":["Ex-3-Task-2"]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Laplace Approximation\n\nOne way of solving this intractability is using __Laplace Approximation__. Discussing about laplace transformation in-depth is beyond the scope of ML level 2 course. If you are interested, go through the book \"Pattern Recognition and Machine Learning\" by Christopher M. Bishop, page 213-216.\n\nIn short, laplace approximation finds the Gaussian approximation to a probability density function.\nWhere, \n- The __mean__ is equal to __mode of the pdf or log transformed pdf__, as log transformation is monotonically increasing function.\n- The __covariance__ is equal to __inverse of hessian matrix of negative log transformed pdf__.\n\nWe use laplace approximation to __find the Gaussian approximation for the posterior distribution__ of logistic regression model and then compute the predictive distribution accordingly.\n\nThe first step of bayesian inference of parameters is assumption of the prior distribution.","metadata":{"deletable":false,"id":"ypXQeG4FDFQv","nbgrader":{"cell_type":"markdown","checksum":"9128bfe00b3cc156dccf581f23de345f","grade":false,"grade_id":"cell-26bf2270658b5dda","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"markdown","source":"## Selecting prior distribution\n\nSince, we are finding Gaussian approximation of the posterior distribution, let's assume a __Gaussian prior__ of form:\n\n$$\np(\\boldsymbol \\beta) \\sim \\mathcal{N}(\\boldsymbol \\beta \\vert \\mathbf{m}_0, \\mathbf{S}_0)\n$$\n\nThe formula being:\n\n$$\n\\boxed{p(\\boldsymbol \\beta) = \\frac{1}{(2\\pi)^{m/2}(|\\mathbf{S}_0|)^{1/2}}\\exp(-\\frac{1}{2}(\\boldsymbol \\beta-\\mathbf{m}_0)^T\\mathbf{S}_0^{-1}(\\boldsymbol \\beta-\\mathbf{m}_0))} \\tag{6}\n$$\n\nwhere, \n$ \\mathbf{m}_0 = \\begin{pmatrix}\nm_0 \\\\\nm_1 \\\\\n\\vdots \\\\\nm_m \\\\\n\\end{pmatrix}_{\\text{((m+1)$\\times$1)}}$,\n$\\mathbf{S}_0 = \\begin{pmatrix}\nS_{00} & S_{01} & \\cdots & S_{0m} \\\\\nS_{10} & S_{11} & \\cdots & S_{1m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nS_{m0} & S_{m1} & \\cdots & S_{mm} \\\\\n\\end{pmatrix}_{\\text{((m+1)$\\times$(m+1))}}$ are the mean vector and covariance matrix of the prior distribution respectively.","metadata":{"deletable":false,"id":"ldOwlNiVDFQv","nbgrader":{"cell_type":"markdown","checksum":"50cdde32e71aae0b26642ca06cd965e8","grade":false,"grade_id":"cell-090011bbc11c6224","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"markdown","source":"### Exercise 4: Specify paramaters of prior distribution\n\n<b><div style=\"text-align: right\">[POINTS: 1]</div></b>\n\n**Task:** \n- Initialize the paramaters as mean vector ```m_0``` and covariance matrix ```S_0``` of the prior distribution such that it is __multivariate standard normal__. Note: assume __features are uncorrelated__, this will affect the covariance matrix.\n\nNote: The shape of the mean vector and covariance matrix $\\mathbf{m}_0$ must be ```(m+1,1)``` and ```(m+1,m+1)```. \n\nHint: Use ```.shape``` function to extract dimension of padded input feature. This will be useful for initializing parameters of the prior distribution.","metadata":{"deletable":false,"id":"sl4nJjW9DFQw","nbgrader":{"cell_type":"markdown","checksum":"6aa5f191a974f06ab9df24f2abd2b4ff","grade":false,"grade_id":"cell-077b0baf29ac0cab","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"# Variables to store mean vector and covariance matrix.\nm_0, S_0 = None, None","metadata":{"deletable":false,"id":"J37ea0KTDFQx","nbgrader":{"cell_type":"code","checksum":"8d0209d29161391b016b9c0981c0daef","grade":false,"grade_id":"cell-8b234b052e629930","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Ex-4-Task-1\n\n### BEGIN SOLUTION\n# your code here\nsiz = X_mat_pad.shape[1]\nm_0 = np.zeros(siz)\nval = np.ones(siz)\nS_0 = np.diag(val)\n#raise NotImplementedError\n### END SOLUTION\n","metadata":{"deletable":false,"id":"Pk7NX1QQDFQ0","nbgrader":{"cell_type":"code","checksum":"675d55c8a6221f6a16ce0ca936bf5daa","grade":false,"grade_id":"cell-7171b8eb5e6d4cec","locked":false,"schema_version":3,"solution":true},"tags":["Ex-4-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert m_0 is not None and S_0 is not None, 'Initialize mean vector and covariance matrix'\n","metadata":{"deletable":false,"id":"Jgef0cLIDFQ3","nbgrader":{"cell_type":"code","checksum":"3cda10789c71327e1b3ccac1ec655bac","grade":true,"grade_id":"cell-b8d5488fdb089e9d","locked":true,"points":1,"schema_version":3,"solution":false},"tags":["Ex-4-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculating the posterior distribution\n\nNext step comprehends the calculation of the posterior distribution of logistic regression model. Using equation 2, 5 and 6, we get the posterior distribution as:\n\n$$\np(\\boldsymbol \\beta \\vert \\mathcal{D}) = \\frac{\\prod_{i=1}^nh_i^{y_i}(1-h_i)^{1-y_i} \\times \\frac{1}{(2\\pi)^{m/2}(|\\mathbf{S}_0|)^{1/2}}\\exp(-\\frac{1}{2}(\\boldsymbol \\beta-\\mathbf{m}_0)^T\\mathbf{S}_0^{-1}(\\boldsymbol \\beta-\\mathbf{m}_0))}{\\int_{\\boldsymbol \\beta' \\in B} p(\\mathcal D \\vert \\boldsymbol \\beta' ) p(\\boldsymbol \\beta') d\\boldsymbol \\beta'}\n$$\n\n## Approximating posterior distribution using laplace approximation\n\nLaplace approximation requires finding log-transformation of the posterior distribution. The resulting equation is in the form:\n\n$$\n\\boxed{\\ln p(\\boldsymbol \\beta \\vert \\mathcal{D}) = -\\frac{1}{2}(\\underbrace{\\boldsymbol \\beta - \\mathbf{m}_0)^T \\mathbf{S}_0^{-1}(\\boldsymbol \\beta - \\mathbf{m}_0}_{w^TAw \\ \\text{form}}) + \\underbrace{\\sum_{i=1}^n\\{y_i \\ln h_i + (1-y_i) \\ln (1-h_i)\\}}_{\\text{vector-matrix form} \\ : \\ \\mathbf{h}^T\\mathbf{y}+(1-\\mathbf{h})^T(1-\\mathbf{y})}  + \\text{const}} \\tag{7}\n$$\n\nWhere, $\\mathbf{h}=\\begin{pmatrix}\nh_1 \\\\\nh_2 \\\\\n\\vdots \\\\\nh_n\n\\end{pmatrix}_{\\text{(n x 1)}}$\n\nThe constant term is irrelavent for finding the mean and covaraince. Now, let's compute the mean and variance of the Gaussian approximation. \n\n- The mean is the mode of $\\ln p(\\boldsymbol \\beta \\vert \\mathcal{D})$. If you recall finding the mode of a pdf is equivalent to __MAP estimation__. So,\n\n$$\n\\boxed{\\mathbf{m}_N = \\boldsymbol \\beta_{\\text{MAP}}} \\tag{8}\n$$\n\n- The covariance is equal to the inverse of the hessian matrix of negative log-posterior distribution. Solving we get the covariance as:\n\n$$\n\\mathbf{S}_N = -\\nabla \\nabla \\ln p(\\boldsymbol \\beta \\vert \\mathcal{D}) = \\mathbf{S}_0^{-1} + \\sum_{i=1}^n h_i(1-h_i)\\mathbf{x}\\mathbf{x}^T $$\n\n$$\n\\boxed{\\mathbf{S}_N =\\underbrace{\\mathbf{S}_0^{-1} + X^T W X}_{\\text{vector-matrix form}}}\\tag{9}\n$$\n\nWhere, $W = \\begin{pmatrix}\nh_1(1-h_1) & 0 & \\cdots & 0 \\\\\n0 &h_2(1-h_2) & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & \\cdots & h_n(1-h_n) \n\\end{pmatrix}_{\\text{(n x n)}}$, is a diagonal matrix with $h_i(1-h_i)$ diagonal element on $i^{th}$ row.\n\nTherefore, the Gaussian approximation to the posterior distribution is:\n\n$$\n\\boxed{q(\\boldsymbol \\beta) \\sim \\mathcal{N}(\\boldsymbol \\beta \\vert \\boldsymbol \\beta_{\\text{MAP}},\\mathbf{S}_N)} \\tag{10}\n$$\n\nFinding the parameters of the Gaussian approximated posterior requires implementation of multiple functions, say for finding the MAP estimate and covariance feature matrix.","metadata":{"deletable":false,"id":"hkQQ57WNDFQ5","nbgrader":{"cell_type":"markdown","checksum":"3cb0690f7fd5845c074c10ef6a29450e","grade":false,"grade_id":"cell-057566a1b7b8d262","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"markdown","source":"### Exercise 5: Define function to calculate negative log posterior function.\n\n<b><div style=\"text-align: right\">[POINTS: 3]</div></b>\n\nFor finding mean of posterior, MAP estimate. For that we require defining the negative log posterior that will be passed to ```scipy.optimize.minimize``` (as we did in programming of MLE).\n\n**Task:**\n- Define function ```calculate_neg_log_posterior``` that:\n 1. Computes the hypothesis given input feature and weight coefficients.\n 2. Computes the 1st and 2nd term present in R.H.S of equation 7 (No need to worry about the constant term). \n 3. Adds the two terms to compute the log posterior distribution and returns the __negative__ log posterior distribution.\n \nNote:\n-  ```scipy.optimize.minimize``` reshapes beta parameter to (m+1,) automatically. You must __reshape the beta parameter into shape (m+1,1) at the start of the function__. \n- __Compute $\\ln(x+\\epsilon)$ instead of $\\ln(x)$__ when implementing the function, where $\\epsilon$ is minute value. This is done such that $log(0)$ doesn't occur.\n- __Use dot product for vectorized computations__ of terms. Refrain from using sums or loops.\n- The __log-posterior value must be in shape ```(1,)```__ containing a single float value. If the shape of log-posterior is ```(1,1)``` use ```np.ravel()```.","metadata":{"deletable":false,"id":"fVP1h9lADFQ6","nbgrader":{"cell_type":"markdown","checksum":"37f2fe67631f6786cdc32fb9c95dc215","grade":false,"grade_id":"cell-fb0a8fab4c0eb67c","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"# Epsilon value (Use when computing the log transformed value)\neps = 1e-10","metadata":{"deletable":false,"id":"367oPqdzDFQ6","nbgrader":{"cell_type":"code","checksum":"e27e33db5c8e42d364b7ac3336cf5145","grade":false,"grade_id":"cell-dc775e67b334999e","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Ex-5-Task-1\n\ndef calculate_neg_log_posterior(beta, input_feat, mean_prior, cov_prior, true_label):\n    '''\n    Computes the negative log posterior of logistic regression model\n    \n    Parameters\n    ----------\n    beta: np.array, shape: (m+1,1)(when used in scipy.optimize.minimize, shape:(m+1,))\n            weight coefficients vector\n            \n    input_feat: np.array, shape:(n,m+1)\n                padded input matrix\n    \n    mean_prior: np.array, shape:(m+1,)\n                mean vector of the prior distribution\n    \n    cov_prior: np.array, shape:(m+1,m+1)\n                covariance matrix of the prior distribution\n                \n    true_label: np.array, shape: (n,1)\n                vector of true label (y)\n    Returns\n    -------\n    neg_log_posterior: np.array, shape:(1,)\n                        negative log posterior of logistic regression\n    '''\n    ### BEGIN SOLUTION\n    # your code here\n    beta = beta.reshape(-1,1)\n    hypothesis = calculate_hypothesis(input_feat, beta)\n    value_first_part = np.dot(np.dot((beta-mean_prior).T,np.linalg.inv(cov_prior)),(beta-mean_prior))\n    value_first_part = (-0.5)*value_first_part\n    value_second_part = (true_label*(np.log(hypothesis + eps))) + ((1-true_label)*(np.log(1-hypothesis + eps)))\n    value_second_part2 = np.sum(value_second_part,axis=0)\n    value = (-1)*(value_first_part + value_second_part2)\n    neg_log_posterior = np.ravel(value)\n    #raise NotImplementedError\n    ### END SOLUTION\n    \n    return neg_log_posterior\n\n","metadata":{"deletable":false,"id":"XyNgjr1hDFQ9","nbgrader":{"cell_type":"code","checksum":"6e94718c0cac2c75451d1bce3490c6f4","grade":false,"grade_id":"cell-0ac2cce82b516941","locked":false,"schema_version":3,"solution":true},"tags":["Ex-5-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert calculate_neg_log_posterior(np.ones((3,1)), np.ones((5,3)), np.ones((3,1)),\n                                   np.identity(3), np.ones((5,1))).shape == (1,),\\\n        'Wrong dimension of negative log posterior'\n","metadata":{"deletable":false,"id":"kr_JRO0UDFQ_","nbgrader":{"cell_type":"code","checksum":"bf1b4b7d413d013737dd2a035eda1d43","grade":true,"grade_id":"cell-ecd5c796a0530809","locked":true,"points":9,"schema_version":3,"solution":false},"tags":["Ex-5-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gradient of negative log posterior\n\nAs stated previously, we will be using scipy's ```optimize.minimize``` function to find the MAP estimate, which is equal to mean of the Gaussian approximated posterior. In this programming, the method used for minimization is __Broyden–Fletcher–Goldfarb–Shanno(BFGS)__ algorithm. Discussing BFGS is beyond the scope of this course. If you want learn more, click [here]( https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) to direct to BFGS wikipedia page.\n\nMain point is BFGS requires __normalized gradient__ of the function, which is being solved. Therefore, we require finding the normalized gradient of negative log posterior. The gradient of the negative log posterior is:\n\n$$\n\\boxed{-\\nabla\\ln p(\\boldsymbol \\beta \\vert \\mathcal{D}) = \\mathbf{S}_0^{-1}(\\boldsymbol \\beta - \\mathbf{m}_0) + X^T(\\mathbf{h} - \\mathbf{y})} \\tag{11}\n$$\n\nIf you have confusion regarding equation 11, refer to reads of MLE for multivariate Gaussian and logistic regression to see how 1st and 2nd term derived, respectively. Next step is to normalize the gradient matrix that will be conducted using ```sklearn``` package.\n","metadata":{"deletable":false,"id":"WjqS0SRqDFRB","nbgrader":{"cell_type":"markdown","checksum":"1c1b26626c9bbc2b50166eaf65f0db53","grade":false,"grade_id":"cell-3384aedee1abcbfc","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"markdown","source":"### Exercise 6: Define function to calculate the normalized gradient of the Gaussian approximated posterior distribution.\n\n<b><div style=\"text-align: right\">[POINTS: 3]</div></b>\n\n__Tasks:__\n- Define a function named ```calculate_normalized_gradient``` that: \n 1. Computes the hypothesis given given input feature and weight coefficients.\n 2. Computes the gradient vector, using equation 11.\n     - The gradient vector shape is ```(m+1,1)```.\n - Finally, computes normalizes the gradient vector and returns it.\n     - Note: We are using ```sklearn.preprocessing.normalize``` to normalize the vector that require a row vector for proper normalization. Be sure to __convert the dimension of gradient vector to ```(1,m+1)``` before normalizing__.\n     - However, the __final dimension of the normalized gradient vector must be of shape ```(m+1,)```__. Use ```np.ravel()``` at the end.\n     \nNote:\n- Reshape beta to shape ```(m+1,1)``` at the start like you did in the previous exercise.","metadata":{"deletable":false,"id":"75unyISJDFRC","nbgrader":{"cell_type":"markdown","checksum":"abeafd26873551aec12239563a6a58d1","grade":false,"grade_id":"cell-b2224f66f3f98102","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"### Ex-6-Task-1\n\nfrom sklearn.preprocessing import normalize\n\ndef calculate_normalized_gradient(beta, input_feat, mean_prior, cov_prior, true_label):\n    '''\n    Computes the covariance of the Gaussian approximated posterior distribution\n    \n    Parameters\n    ----------\n    beta: np.array, shape: (m+1,1)(when used in scipy.optimize.minimize, shape:(m+1,))\n            weight coefficients vector\n            \n    input_feat: np.array, shape:(n,m+1)\n                padded input matrix\n    \n    mean_prior: np.array, shape:(m+1,)\n                mean vector of the prior distribution\n    \n    cov_prior: np.array, shape:(m+1,m+1)\n                covariance matrix of the prior distribution\n                \n    true_label: np.array, shape: (n,1)\n                vector of true label (y)\n                \n    Returns\n    -------\n    norm_grad_neg_log_post: np.array, shape:(m+1,)\n                            normalized gradient vector of negative log post\n    '''\n    ### BEGIN SOLUTION\n    # your code here\n    beta = beta.reshape(-1,1)\n    hypothesis = calculate_hypothesis(input_feat, beta)\n    gradient_first_part = np.dot(np.linalg.inv(cov_prior),(beta-mean_prior))\n    gradient_second_part = np.dot(input_feat.T,(hypothesis-true_label))\n    gradient = gradient_first_part + gradient_second_part\n    gradient = gradient.T\n    norm_grad_neg_log_post = normalize(gradient)\n    norm_grad_neg_log_post = norm_grad_neg_log_post.T\n    norm_grad_neg_log_post = norm_grad_neg_log_post.ravel()\n    #raise NotImplementedError\n    ### END SOLUTION\n    \n    return norm_grad_neg_log_post\n\n","metadata":{"deletable":false,"id":"_Njw-uD4DFRD","nbgrader":{"cell_type":"code","checksum":"cdb8831cdce5db6dad09b0b57738dc1e","grade":false,"grade_id":"cell-944597c07907b6c6","locked":false,"schema_version":3,"solution":true},"tags":["Ex-6-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert calculate_normalized_gradient(np.ones((3,1)), np.ones((5,3)), np.ones((3,1)),\n                                   np.identity(3), np.ones((5,1))).shape == (3,),\\\n        'Wrong dimension of normalized gradient'\n","metadata":{"deletable":false,"id":"oM1KjBCdDFRG","nbgrader":{"cell_type":"code","checksum":"6b7051dba196135e32f5ac6a73f29933","grade":true,"grade_id":"cell-f02adb5c7134cd27","locked":true,"points":3,"schema_version":3,"solution":false},"tags":["Ex-6-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exercise 7: Define function to calculate the covariance matrix of the Gaussian approximated posterior distribution.\n\nNext we require function to compute the covariance matrix.\n\n<b><div style=\"text-align: right\">[POINTS: 2]</div></b>\n\n**Task:**\n- Define a function named ```calculate_posterior_covariance``` that:\n 1. Computes the hypothesis, given input feature and weight coefficients.\n 2. Computes the W diagonal matrix present in R.H.S of equation 9. \n 3. Finally, computes the covariance and returns it.","metadata":{"deletable":false,"id":"4AKO_GHoDFRJ","nbgrader":{"cell_type":"markdown","checksum":"f4ab304bc42f1c2870cfca6b950b4e28","grade":false,"grade_id":"cell-fac2846bb43ad27a","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"### Ex-7-Task-1\n\ndef calculate_posterior_covariance(beta, input_feat, cov_prior):\n    '''\n    Computes the covariance of the Gaussian approximated posterior distribution\n    \n    Parameters\n    ----------\n    beta: np.array, shape: (m+1,1)\n            weight coefficients vector\n            \n    input_feat: np.array, shape:(n,m+1)\n                padded input matrix\n    \n    cov_prior: np.array, shape:(m+1,m+1)\n                covariance matrix of the prior distribution\n    Returns\n    -------\n    cov: np.array, shape:(m+1, m+1)\n         covariance of the Gaussian approxmated posterior\n    '''\n    ### BEGIN SOLUTION\n    # your code here\n    hypothesis = calculate_hypothesis(input_feat, beta)\n    val = hypothesis*(1-hypothesis)\n    val = val.reshape(-1,)\n    var = np.diag(val)\n    cov = np.linalg.inv(cov_prior) + np.dot(np.dot(input_feat.T,var),input_feat)\n    #raise NotImplementedError\n    ### END SOLUTION\n    return cov\n\n","metadata":{"deletable":false,"id":"vRQTrPSfDFRJ","nbgrader":{"cell_type":"code","checksum":"487339363aff2804eb10ff5dd3b68da1","grade":false,"grade_id":"cell-e994932289c3bd6c","locked":false,"schema_version":3,"solution":true},"tags":["Ex-7-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert calculate_posterior_covariance(np.ones((3,1)), np.ones((5,3)),\n                                   np.identity(3)).shape == (3,3),\\\n        'Wrong dimension of covariance matrix'\n\n","metadata":{"deletable":false,"id":"GPIkuOT4DFRM","nbgrader":{"cell_type":"code","checksum":"8b0ea0ae33ebfd42fd024450b1e83749","grade":true,"grade_id":"cell-214d2ee7e2825c3f","locked":true,"points":2,"schema_version":3,"solution":false},"tags":["Ex-7-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exercise 8: Define function to compute the mean and variance of Gaussian approximate posterior distribution.\n\nUsing both functions defined from exercise 5 to 7 compute the mean and varaince of Gaussian approximate posterior distribution.\n<b><div style=\"text-align: right\">[POINTS: 2]</div></b>\n\n__Tasks__:\n - Define a function named ```calculate_approx_params``` that:\n   1. Computes the mean of the Gaussian approximate posterior and store it in variable ```mean_post```.\n     - Use ```scipy.optimize.minimize``` to optimize the negative log posterior function, __use method ```BFGS```__. Note: __```jac``` parameter must be the normalized gradient calculating function__.\n     - Note: ```scipy.optimize.minimize``` returns a dictionary, where the parameter that minimizes the objective function is stored as value of key `x`(Map estimate). __Reshape the MAP estimate to shape (m+1,1)__.\n   2. Computes the covariance of the Gaussian approximate posterior and store it in variable ```cov_post```.\n   3. Finally, return both variable ```mean_post, cov_post```.","metadata":{"deletable":false,"id":"Lqyb-NV8DFRO","nbgrader":{"cell_type":"markdown","checksum":"2c6e0838849b6ce26efbd39401a83036","grade":false,"grade_id":"cell-e43a7fafb7ffa01d","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"### Ex-8-Task-1\n\nfrom scipy import optimize\n\ndef calculate_approx_params(beta, input_feat, mean_prior, cov_prior, true_label):\n    '''\n    Computes the parameters of the Gaussian approximated posterior distribution\n    \n    Parameters\n    ----------\n    beta: np.array, shape: (m+1,1)\n            weight coefficients vector\n            \n    input_feat: np.array, shape:(n,m+1)\n                padded input matrix\n    \n    mean_prior: np.array, shape:(m+1,)\n                mean vector of the prior distribution\n    \n    cov_prior: np.array, shape:(m+1,m+1)\n                covariance matrix of the prior distribution\n                \n    true_label: np.array, shape: (n,1)\n                vector of true label (y)\n                \n    Returns\n    -------\n    mean_post: np.array, shape:(m+1,1)\n               MAP estimate, mean of Gaussian approximated posterior\n    \n    cov_post: np.array, shape:(m+1,m+1)\n              Covariance of Gaussian approximated posterior\n                \n    '''\n    ### BEGIN SOLUTION\n    # your code here\n    mean_post = optimize.minimize(calculate_neg_log_posterior,beta,args=(input_feat,mean_prior,cov_prior,true_label),method='BFGS', jac=calculate_normalized_gradient)\n    cov_post = calculate_posterior_covariance(mean_post,input_feat,cov_prior)\n    #raise NotImplementedError\n    ### END SOLUTION\n    return mean_post,cov_post","metadata":{"deletable":false,"id":"BfS94waNDFRO","nbgrader":{"cell_type":"code","checksum":"48008988978fe7a47c6482377cea3fdf","grade":false,"grade_id":"cell-090c8a09a43f485e","locked":false,"schema_version":3,"solution":true},"tags":["Ex-8-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert calculate_approx_params(np.ones((3,1)), np.ones((5,3)), np.ones((3,1)),\n                                   np.identity(3), np.ones((5,1)))[0].shape == (3,1),\\\n                                'wrong shape of mean vector'\n\nassert calculate_approx_params(np.ones((3,1)), np.ones((5,3)), np.ones((3,1)),\n                                   np.identity(3), np.ones((5,1)))[1].shape == (3,3),\\\n                                'wrong shape of covariance matrix'\n","metadata":{"deletable":false,"id":"bZM9kjr_DFRQ","nbgrader":{"cell_type":"code","checksum":"b424316a5bac5414297eebc8c5f7d4ff","grade":true,"grade_id":"cell-b4f641eba1e759da","locked":true,"points":2,"schema_version":3,"solution":false},"tags":["Ex-8-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predictive distribution\n\nNow that you have obtained the gaussian approximation to posterior distribution, next step is marginalizing with respect to this distribution for making predictions.\n\n#### Marginalizing with respect to posterior distribution\n\nThe predictive distribution for class $C=1$ for given input feature vector $\\mathbf{x}$, is obtained by marginalizing w.r.to Gaussian approximated posterior.\n\n$$p(C=1 \\vert \\mathbf{x},\\mathbf{y}) = \\int p(C=1 \\vert \\mathbf{x}, \\boldsymbol \\beta)p(\\boldsymbol \\beta \\vert \\mathbf{y})d\\boldsymbol \\beta = \\int \\sigma(\\mathbf{x}^T \\boldsymbol \\beta)q(\\boldsymbol \\beta)d\\boldsymbol \\beta\\tag{12} \n$$ \n\nThe probability for class $C=0$ is $p(C=0 \\vert \\mathbf{x},\\mathbf{y}) = 1-p(C = 1 \\vert \\mathbf{x},\\mathbf{y})$. \n\nTo evaluate this predictive distribution, equation 12, we first recall that hypothesis $\\sigma(\\mathbf{x}^T \\boldsymbol \\beta)$ depends on $\\boldsymbol \\beta$ through its projection onto $X$. Mathematically,\n\n$$\n\\sigma(\\mathbf{x}^T \\boldsymbol \\beta) = \\int \\delta(a- \\mathbf{x}^T \\boldsymbol \\beta)\\sigma(a)da \\tag{13}\n$$\n\nWhere, $a = \\mathbf{x}^T \\boldsymbol \\beta$, $\\delta(.)$ is the Dirac delta function. From equation 12 and 13, we get:\n\n$$\n\\int \\sigma(\\mathbf{x}^T \\boldsymbol \\beta)q(\\boldsymbol \\beta)d\\boldsymbol \\beta = \\int \\sigma(a)p(a)da \\tag{14}\n$$\n\nWhere, $p(a)=\\int \\delta(a- \\mathbf{x}^T \\boldsymbol \\beta)q(\\boldsymbol \\beta)d\\boldsymbol \\beta $\n\nWe can evaluate $p(a)$ by noting that the delta function imposes a linear constraint on w and so forms a marginal distribution from the joint distribution $q(\\boldsymbol \\beta)$ by integrating out all directions orthogonal to $X$. Because q(w) is Gaussian, so the marginal distribution will also be Gaussian. We can evaluate the mean and covariance of $p(a)$ distribution by taking moments, and interchanging the order of integration over a and w. (Bishop C. M., 2006). So the mean($\\mu_a$) and variance ($v_a^2$) of $p(a)$ per feature vector is as:\n\n$$\\boxed{\\mu_a = \\mathbf{x}^T \\beta_{\\text{MAP}}} \\tag{15}$$\n\n$$\\boxed{v_a^2 = \\mathbf{x}^T \\mathbf{S}_N \\mathbf{x}} \\tag{16}$$\n\nSo the approximation of the predictive distribution is:\n\n$$p(C_1 \\vert \\mathbf{x},\\mathbf{y}) = \\int{\\sigma(a)p(a) da} = \\int \\sigma(a) \\mathcal{N}(a \\vert \\mu_a,v_a^2) da \\tag{17} $$\n\nEquation 17 cannot be evaluated analytically because of the presence of sigmoid function. We can solve this intractability by approximating the sigmoid function with __re-scaled probit function__,  $\\Phi(\\lambda a)$. In short, probit function is the cumulative distribution function of a standard normal Gaussian and has similar shape to sigmoid function. Here, approximating sigmoid with probit function requires both of the function to have same slope at the origin, meaning the scaling factor of probit function, $\\lambda = \\pi/8$(Bishop C. M., 2006). \n\nTo know more about probit transformation, read section 4.3.5 Probit regression, \"Pattern Recognition and Machine Learning\" by Christopher M. Bishop. \n\nThe advantage of using probit function is that its convolution with a Gaussian can be expressed as another probit function. Specifically, the formula for expressing convolution as another probit is given as:\n\n$$\\int \\Phi(\\lambda a)\\mathcal{N}(a \\vert \\mu,v^2) da = \\Phi (\\frac{\\mu}{(\\lambda^{-2}+ v^2)^{1/2}})  \\tag{18}$$\n\nApplying approximation of $\\sigma(a) \\approx \\Phi(\\lambda a) $ equation can be rewritten as:\n\n$$\\int \\sigma(a) \\mathcal{N}(a \\vert \\mu,v^2) da \\approx \\sigma(\\kappa ( v^2)\\mu) \\tag{19}$$\n\nWhere, $$\\boxed{\\kappa(  v^2) = (1+\\pi  v^2/8)^{-1/2}} \\tag{20}$$ \n\nFinally, from equation 17 and 19, we find the predictive distribution as:\n\n$$\\boxed{p(C=1 \\vert \\mathbf{x},\\mathbf{y}) = \\sigma(\\kappa (\\sigma_a^2) \\mu_a)} \\tag{21}$$","metadata":{"deletable":false,"id":"LeMRa_goDFRT","nbgrader":{"cell_type":"markdown","checksum":"ec95a7249e5455d2191b01911c091c1d","grade":false,"grade_id":"cell-062125e25c228e17","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"markdown","source":"### Exercise 9:  Define function to compute the prediction (positive class probability) of bayesian logistic regression model.\n\n<b><div style=\"text-align: right\">[POINTS: 3]</div></b>\n\n- __Tasks__:\n - Define function named ```prediction``` that: __[Task 2: Points 2]__\n   1. Compute the __mean and variance vectors__ (not single values) of distribution $p(\\mathbf{a})$ and stores it in variables ```mu_a, v2_a```, respectively.\n    - Use vectorization for computation of mean vector.\n    - Vectorization for computing variance vector is cumbersome, instead\n        - Define function ```calculate_v2``` that calculates the sample wise variance. __[Task 1: Points 1]__\n        - Use ```numpy.apply_along_axis``` function to pass row slice (data sample) to ```calculate_v2``` function for computing the variance vector.\n   2. Compute the $\\kappa(v^2)$  vector and store it in variable ```kappa_v2_a```.\n   3. Finally compute the prediction vector for C=1 class and return the prediction, stored in variable ```predicted_prob```. (Hint: Just multiply ```*``` $\\kappa$ and mean vector, don't perform matrix multiplication).\n   \nNote: the shape of mean, variance, kappa and prediction vector should be ```(n,1)```. Reshape the vectors as required.","metadata":{"deletable":false,"id":"BIb9JwWBDFRT","nbgrader":{"cell_type":"markdown","checksum":"a5aee37c376f1e3b7d2fc2e6f23613e6","grade":false,"grade_id":"cell-4501a39b7d7d8a11","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"### Ex-9-Task-1\ndef calculate_v2(input_feat, cov_post):\n    '''\n    Computes the sample wise variance of the p(a)\n    \n    Parameters\n    ----------     \n    input_feat: np.array, shape:(m+1,)\n                padded input feature vector\n    \n    cov_post: np.array, shape:(m+1,m+1)\n              Covariance of Gaussian approximated posterior\n                \n    Returns\n    -------\n    samp_v2: np.float\n             Variance of p(a) per sample\n    \n    '''\n    ### BEGIN SOLUTION\n    # your code here\n    raise NotImplementedError\n    ### END SOLUTION\n    return samp_v2","metadata":{"deletable":false,"id":"-ZQWO-IfDFRU","nbgrader":{"cell_type":"code","checksum":"dc6d8052b9257daab30466efdb7acec9","grade":false,"grade_id":"cell-9d09a76997a084e4","locked":false,"schema_version":3,"solution":true},"tags":["Ex-9-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert type(calculate_v2(np.ones(3), np.ones((3,3)))) == np.float64, 'Wrong format of variance'\n","metadata":{"deletable":false,"id":"pTHee3VDDFRW","nbgrader":{"cell_type":"code","checksum":"51802900680a8be1fc8d18470cc69cde","grade":true,"grade_id":"cell-68e4547c4f629316","locked":true,"points":1,"schema_version":3,"solution":false},"tags":["Ex-9-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Ex-9-Task-2\n\ndef prediction(input_feat, mean_post, cov_post):\n    '''\n    Computes the parameters of the Gaussian approximated posterior distribution\n    \n    Parameters\n    ----------\n            \n    input_feat: np.array, shape:(n,m+1)\n                padded input matrix\n    \n    mean_post: np.array, shape:(m+1,1)\n               MAP estimate, mean of Gaussian approximated posterior\n    \n    cov_post: np.array, shape:(m+1,m+1)\n              Covariance of Gaussian approximated posterior\n                \n    Returns\n    -------\n    predicted_prob: np.array, shape:(n,1)\n                    Probability of the feature being of class=1.\n                \n    '''\n    ### BEGIN SOLUTION\n    # your code here\n    raise NotImplementedError\n    ### END SOLUTION\n    \n    return predicted_prob","metadata":{"deletable":false,"id":"njMBgkj-DFRa","nbgrader":{"cell_type":"code","checksum":"3546857a781d9cdc375f9c19d32404af","grade":false,"grade_id":"cell-7cf4cd2f184b9663","locked":false,"schema_version":3,"solution":true},"tags":["Ex-9-Task-2"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert prediction(np.ones((5,2)), np.ones((2,1)), np.identity(2)).shape == (5,1), 'wrong dimension of prediction vector'\n","metadata":{"deletable":false,"id":"LSxcToGaDFRd","nbgrader":{"cell_type":"code","checksum":"266251f23ecb280f3e57ef7a1ec64ef0","grade":true,"grade_id":"cell-27b4d9763d8981a1","locked":true,"points":2,"schema_version":3,"solution":false},"tags":["Ex-9-Task-2"]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exercise 10: Train the model and compute predictions.\n\n<b><div style=\"text-align: right\">[POINTS: 2]</div></b>\n\n__Tasks__:\n 1. Initialize guess of weight ```beta_initial``` where all elements equal to 1. Then, Train the model using training set. i.e. find the parameters of the posterior distribution and store the mean and covariance in ```m_N, S_N``` respectively. __[Task 1: points:1]__\n 2. Compute the predictions for both train and test size. Store the predictions in ```y_pred_train, y_pred_test``` for training and test set respectively. Note: ```prediction``` function returns the probability not the prediction. Use condition to compute the prediction, for example: some threshold. __[Task 2: points:1]__","metadata":{"deletable":false,"id":"lIDPMryKDFRg","nbgrader":{"cell_type":"markdown","checksum":"569b3271fc64c8fac26f4a44dc588b5a","grade":false,"grade_id":"cell-5a0cebd8881f720d","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"# Variable to store initial guess of weight coefficient\nbeta_initial = None\n\n# Variable to store parameters of posterior distribution\nm_N, S_N = None, None\n\n# Variable to store predictions\ny_pred_train, y_pred_test = None, None","metadata":{"deletable":false,"id":"mMLOrlj5DFRh","nbgrader":{"cell_type":"code","checksum":"a7a475aa7a27994634a51ff8f55e3bb2","grade":false,"grade_id":"cell-d47d6c48e3b66203","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Ex-10-Task-1\n\n### BEGIN SOLUTION\n# your code here\nraise NotImplementedError\n### END SOLUTION","metadata":{"deletable":false,"id":"abaoAAbSDFRj","nbgrader":{"cell_type":"code","checksum":"42a22eb113e227799db1b956fa201acc","grade":false,"grade_id":"cell-68921ae1f1a5515c","locked":false,"schema_version":3,"solution":true},"tags":["Ex-10-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert beta_initial is not None, 'Initialize guess of weight coefficient'\nassert m_N is not None and S_N is not None, 'Compute mean vector and covariance matrix'\n","metadata":{"deletable":false,"id":"98C_6u9wDFRn","nbgrader":{"cell_type":"code","checksum":"48686fc31f99f6f2e4a9d64ca614b925","grade":true,"grade_id":"cell-17ece03102678e36","locked":true,"points":1,"schema_version":3,"solution":false},"tags":["Ex-10-Task-1"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Ex-10-Task-2\n\n### BEGIN SOLUTION\n# your code here\nraise NotImplementedError\n### END SOLUTION","metadata":{"deletable":false,"id":"kHaHbImaDFRp","nbgrader":{"cell_type":"code","checksum":"1b12cbc31a2146e1375b413f9acb8794","grade":false,"grade_id":"cell-52887a02ea4a3701","locked":false,"schema_version":3,"solution":true},"tags":["Ex-10-Task-2"]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert y_pred_test is not None and y_pred_train is not None, 'Compute predictions for train and test sets'","metadata":{"deletable":false,"id":"H6cGiPlaDFRs","nbgrader":{"cell_type":"code","checksum":"048da86658e2611c4c0375bc3b927a16","grade":true,"grade_id":"cell-1f78d9403a187d3a","locked":true,"points":1,"schema_version":3,"solution":false},"tags":["Ex-10-Task-2"]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation and decision boundary\n\nFirst, let's look at the posterior distribution of the parameters of the model. As we know, plotting the multivariate Gaussian distribution of three features is impossible to visualize (requires four dimensional plot), let's plot 2d univariate Gaussian plot per parameter.","metadata":{"deletable":false,"id":"_Zp93NY_DFRv","nbgrader":{"cell_type":"markdown","checksum":"3cde517a4fb8bc3d7a9a52366ae2c9f2","grade":false,"grade_id":"cell-506ee1f827909d87","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"from scipy import stats\n\n# Mean and Varaince of Beta_0\nmean1 = m_N[0]\nvar1 = S_N[0, 0]\n\n# Mean and Varaince of Beta_1\nmean2 = m_N[1]\nvar2 = S_N[1, 1]\n\n# Mean and Varaince of Beta_2\nmean3 = m_N[2]\nvar3 = S_N[2, 2]\n\n# Generating subplots\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 4))\n\n# Values for plot\nx1_v = np.linspace(mean1-4*np.sqrt(var1), mean1+4*np.sqrt(var1), 100)\nm1_h = stats.norm.pdf(mean1, mean1, np.sqrt(var1))\nx2_v = np.linspace(mean2-4*np.sqrt(var2), mean2+4*np.sqrt(var2), 100)\nm2_h = stats.norm.pdf(mean2, mean2, np.sqrt(var2))\nx3_v = np.linspace(mean3-4*np.sqrt(var3), mean3+4*np.sqrt(var3), 100)\nm3_h = stats.norm.pdf(mean3, mean3, np.sqrt(var3))\n\n# Posterior of beta_0\nax1.plot(x1_v, stats.norm.pdf(x1_v, mean1, np.sqrt(var1)))\nax1.vlines(mean1, 0, m1_h, color='red')\nax1.tick_params(axis='both', labelsize=10)\nax1.set_title(r'Posterior distribution of $\\beta_0$', fontsize=12)\nax1.set_ylim(0)\n\n# Posterior of beta_1\nax2.plot(x2_v, stats.norm.pdf(x2_v, mean2, np.sqrt(var2)))\nax2.vlines(mean2, 0, m2_h, color='red')\nax2.tick_params(axis='both', labelsize=10)\nax2.set_title(r'Posterior distribution of $\\beta_1$', fontsize=12)\nax2.set_ylim(0)\n\n# Posterior of beta_2\nax3.plot(x3_v, stats.norm.pdf(x3_v, mean3, np.sqrt(var3)))\nax3.vlines(mean3, 0, m3_h, color='red')\nax3.tick_params(axis='both', labelsize=10)\nax3.set_title(r'Posterior distribution of $\\beta_2$', fontsize=12)\nax3.set_ylim(0)\n\nplt.plot()","metadata":{"deletable":false,"id":"hZl39f5kDFRw","nbgrader":{"cell_type":"code","checksum":"c153c24cd70525819efb37e5824647e6","grade":false,"grade_id":"cell-4ec310f6350b679c","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above plots show the individual posterior distribution of the parameters of the logistic regression model. The red vertical lines are the mean of the distribution (```m_N```). \n\nFrom the training, the mean and covariance of posterior distribution given the data are:","metadata":{"deletable":false,"id":"9EHPsCKJDFRz","nbgrader":{"cell_type":"markdown","checksum":"236326efdba9ee605be36959c1a5d58c","grade":false,"grade_id":"cell-2479a8d5d30b151b","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"print(f'Mean of posterior distribution:\\n {m_N}')\nprint(f'Covariance of posterior distribution:\\n {S_N}')","metadata":{"deletable":false,"id":"-p38DZ8uDFRz","nbgrader":{"cell_type":"code","checksum":"17ce53d264e31d3fd89236f5ca7bd7aa","grade":false,"grade_id":"cell-97b454ff0feb4667","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's evaluate trained bayesian logistic regression model using ```sklearn.metrics.classification_report```.","metadata":{"deletable":false,"id":"FKmCytS0DFR1","nbgrader":{"cell_type":"markdown","checksum":"6f9df48734ef1a26ce809f192257aa42","grade":false,"grade_id":"cell-d0a7ea29a72de3cd","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint('----------------------------for training---------------------------')\nprint(classification_report(y_train, y_pred_train))\n\nprint('----------------------------for testing---------------------------')\nprint(classification_report(y_test, y_pred_test))","metadata":{"deletable":false,"id":"UgHO2WCMDFR1","nbgrader":{"cell_type":"code","checksum":"625fdef6378b17b7ea374b0a9bc3d6f1","grade":false,"grade_id":"cell-924c7a54e1ff8151","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the synthetic data was linearly separable the model is performing exceptionally with around 98% accuracy.  Now. let's plot the decision boundary of the model.","metadata":{"deletable":false,"id":"1uHr1FxQDFR5","nbgrader":{"cell_type":"markdown","checksum":"a1c5995a302064c6e0156de5dfceb4f8","grade":false,"grade_id":"cell-15f21674177ad6c3","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"# Generating independent features\nx1_points = np.linspace(-0.05,1.05, 1000)\nx2_points = np.linspace(-0.05,1.05, 1000)\n\n# Generating grid of values\nx1,x2 = np.meshgrid(x1_points,x2_points)\n\n# Changing dimension (Required for vectorized operations)\nX_values = np.column_stack((x1.ravel(),x2.ravel()))\n\nX_sample = np.c_[np.ones(X_values.shape[0]),X_values]\n\n# Computing hypothesis function and prediction\ny_pred = prediction(X_sample, m_N, S_N)\ny_pred = y_pred.reshape(x1.shape)\n\n\n# Plotting the datapoints (helps in visualization)\nfig, (ax1,ax2) = plt.subplots(1,2, figsize=(15,6))\nax1.plot(X_mat_pad[:,1][np.ravel(y_vec)==0], X_mat_pad[:,2][np.ravel(y_vec)==0], 'bs', label='class 0')\nax1.plot(X_mat_pad[:,1][np.ravel(y_vec)==1], X_mat_pad[:,2][np.ravel(y_vec)==1], 'g^', label='class 1')\nax1.set_xlabel(syn_df.columns[0], fontsize=12)\nax1.set_ylabel(syn_df.columns[1], fontsize=12)\nax1.set_title('Prediction probability', fontsize=12)\nax1.legend()\n\n# Plotting the prediction probability\nax1.contourf(x1, x2, y_pred, cmap=plt.cm.brg, alpha=0.2)\n\n# Plotting the datapoints (helps in visualization)\nax2.plot(X_mat_pad[:,1][np.ravel(y_vec)==0], X_mat_pad[:,2][np.ravel(y_vec)==0], 'bs', label='class 0')\nax2.plot(X_mat_pad[:,1][np.ravel(y_vec)==1], X_mat_pad[:,2][np.ravel(y_vec)==1], 'g^', label='class 1')\nax2.set_xlabel(syn_df.columns[0], fontsize=12)\nax2.set_ylabel(syn_df.columns[1], fontsize=12)\nax2.set_title('Decision boundary', fontsize=12)\nax1.legend()\n\n# Plotting the prediction probability\nax2.contourf(x1, x2, np.round(y_pred), cmap=plt.cm.brg, alpha=0.2)\nplt.show()","metadata":{"deletable":false,"id":"254VUcgrDFR5","nbgrader":{"cell_type":"code","checksum":"6e07990f55c577d5457deecb384d2e2c","grade":false,"grade_id":"cell-e3658c4c64d8a212","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On the left side is the plot of the prediction probability, aka confidence. The confidence between region of two classes is lower(shown by red) than the confidence near the data cluster(shown by green and blue). On the right we have the decision boundary of the trained model, which as expected is a linear line separating the data.","metadata":{"deletable":false,"id":"HDNw6wE-DFR7","nbgrader":{"cell_type":"markdown","checksum":"e0a1b4b396b3fa3aba3f9d80e7846e54","grade":false,"grade_id":"cell-b5e2a455b6c4968b","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"markdown","source":"## Why feature scaling?\n\nWe conducted feature scaling at the start of this assignment. But why? Let's train the bayesian logistic regression model with using feature scaling and see the results.","metadata":{"deletable":false,"id":"FW4VevYKDFR8","nbgrader":{"cell_type":"markdown","checksum":"250f78395ff9e866a22342d9657d50d9","grade":false,"grade_id":"cell-a0a3212b82dde3b5","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"# Unscaled data\nX_unscaled = np.c_[np.ones(X_data.shape[0]), X_data]\n\nm_N_unscaled, S_N_unscaled = calculate_approx_params(beta_initial, X_unscaled, m_0, S_0, y_vec)\n\ny_unscaled_pred = np.round(prediction(X_unscaled, m_N_unscaled, S_N_unscaled))\n\n# Accuracy\nprint(classification_report(y_vec, y_unscaled_pred))","metadata":{"deletable":false,"id":"YiCIAkXzDFR8","nbgrader":{"cell_type":"code","checksum":"e7fba8ad81b05c68a973d72a1f213766","grade":false,"grade_id":"cell-896efe818ef3b250","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By using unscaled features, the accuracy has dropped from 98% to 76%. Let's check the decision boundary.","metadata":{"deletable":false,"id":"20Skd8__DFR_","nbgrader":{"cell_type":"markdown","checksum":"fb5fa7e3792aab4cb99be5104e3526aa","grade":false,"grade_id":"cell-7ee3c598e0cbef7e","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"code","source":"# Generating independent features\nx1_points = np.linspace(46,54.2, 1000)\nx2_points = np.linspace(46.7,53.2, 1000)\n\n# Generating grid of values\nx1,x2 = np.meshgrid(x1_points,x2_points)\n\n# Changing dimension (Required for vectorized operations)\nX_values = np.column_stack((x1.ravel(),x2.ravel()))\n\nX_sample = np.c_[np.ones(X_values.shape[0]),X_values]\n\ny_unscaled_pred = np.round(prediction(X_sample, m_N_unscaled, S_N_unscaled))\n\n# Plotting the datapoints (helps in visualization)\nplt.plot(X_unscaled[:,1][np.ravel(y_vec)==0], X_unscaled[:,2][np.ravel(y_vec)==0], 'bs', label='class 0')\nplt.plot(X_unscaled[:,1][np.ravel(y_vec)==1], X_unscaled[:,2][np.ravel(y_vec)==1], 'g^', label='class 1')\nplt.xlabel(syn_df.columns[0])\nplt.ylabel(syn_df.columns[1])\nplt.title('Decision boundary')\nplt.legend()\n\n# Plotting the decision boundary\nplt.contourf(x1, x2, y_unscaled_pred.reshape(x1.shape), cmap=plt.cm.brg, alpha=0.2)\nplt.show()","metadata":{"deletable":false,"id":"Y9zYY3bSDFR_","nbgrader":{"cell_type":"code","checksum":"97645e5e48acfbfb05d5fc8d72b7d7e4","grade":false,"grade_id":"cell-db2ccaac4ad5ae15","locked":true,"schema_version":3,"solution":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See the decision boundary is unable to separate the two classes. The BFGS algorithm that we used fails to converge due to unscaled feature, resulting in suboptimal decision boundary. Remember: your data must always be ready for any algorithm, so it is highly advised that you always scale the features. ","metadata":{"deletable":false,"id":"qt9RvhooDFSD","nbgrader":{"cell_type":"markdown","checksum":"625b238277a57c7b073c1e26a6015cbb","grade":false,"grade_id":"cell-b92bbfb17e9efabc","locked":true,"schema_version":3,"solution":false}}},{"cell_type":"markdown","source":"## Additional Resources\n\n- Book\n - Bishop C. M. (2006), Pattern recognition and machine learning, \n   - Check Topic: Chapter 4, Section 4.4, 4.5 contains information regarding laplace approximation and bayesian logistic regression respectively.","metadata":{"deletable":false,"id":"rHGl63ynDFSD","nbgrader":{"cell_type":"markdown","checksum":"a71b532ff3d9026bc5e04dc8d376e4d2","grade":false,"grade_id":"cell-609fdd38741aa904","locked":true,"schema_version":3,"solution":false}}}]}